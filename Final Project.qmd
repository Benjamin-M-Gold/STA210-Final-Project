---
title: "Final Project"
format: pdf
editor: visual
author: "Benji Gold and Sam Alkalay"
---

## Introduction

Baseball, America's pastime, has a long and storied tradition that dates back well over 100 years. Since the 1850's, some form of statistics measuring how good a player is has been tracked. This began through the use of the box score, which tracked basic statistics, such as hits, runs, and errors, from which a player's batting average can be constructed. Over one hundred years later, a pioneering statistician by the name of Bill James introduced new statistical concepts, such as on-base percentage and runs created, in his annual Baseball Abstract (Lee 2018). As technology has improved, the statistics being tracked became more and more sophisticated. Then, in 2015 analytics in baseball took a giant leap. With the introduction of Statcast, teams were able to track novel metrics, such as a batter's exit velocity (the speed of the baseball as it comes off the bat, immediately after a batter makes contact) and barrel percentage (the percentage of baseballs hit off of the player's barrel) ("Statcast Search"). Around the league, teams adopted these new statistics to try and gain a competitive advantage, through which they would be able to better predict a player's potential. However, is this actually the case? While these new statistics are widely used, it is unclear whether they actually provide any useful information for predicting a player's potential. This research project intends to explore that idea through the use of a logistic regression model to predict whether a player is an all-star. The research question of interest is:

Do old or new wave statistics do a better job at predicting whether a player is selected as an all-star?

The response variables of interest are: All.Star: Whether a player is selected as an all-star. Salary: How much money a player makes.

For our analysis, we have selected two datasets. The first is from Baseball Reference, which consists of standard statistics that offer a broad view of a player's performance in a particular season. The second is from Statcast, which consists of each player's primary position. ADD MORE ABOUT WHAT WE DID WITH THE DATA HERE

## Methodology

## Results

## Discussion

## Packages and Data

```{r libs, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels) 
library(glmnet)
library(caret) 
library(MASS)
library(lme4)
stats <- read.csv("data/stats.csv")


stats <- replace(stats, stats =="", NA)
stats <- stats %>%
  drop_na() %>%
  mutate(AVG300 = case_when(batting_avg >= .3 ~ "Greater than 300", TRUE ~ "Less than 300"),
         HR40 = case_when(b_home_run >= 40 ~ "Greater than 40", TRUE ~ "Less than 40"), pitcher = case_when(Position == "SP" ~ "Yes", TRUE ~ "No"))
view(stats)
```

## Lassos for Variable Selection

```{r lasso-basic,warning=FALSE, message=FALSE}
# LASSO Variable Selection Basic Stats
y <- stats$All.Star
x <- model.matrix(All.Star ~ player_age + b_ab + b_total_pa + b_total_hits + b_home_run + AVG300 * batting_avg +
                    b_double + b_triple + b_home_run * HR40 + b_strikeout + b_walk + 
                    batting_avg + slg_percent + on_base_percent + Position, data = stats)
m_lasso_cv <- cv.glmnet(x, y, alpha = 1)
best_lambda <- m_lasso_cv$lambda.min
best_lambda
m_best <- glmnet(x, y, alpha = 1, lambda = best_lambda)
m_best$beta
```

```{r lasso-adv,warning=FALSE, message=FALSE}
# LASSO Variable Selection Advanced Stats
y <- stats$All.Star
x <- model.matrix(All.Star ~ player_age + launch_angle_avg + sweet_spot_percent + 
                    barrel + solidcontact_percent + flareburner_percent + 
                    hard_hit_percent + avg_hyper_speed + z_swing_percent + 
                    oz_swing_percent + meatball_swing_percent, data = stats)
m_lasso_cv <- cv.glmnet(x, y, alpha = 1)
best_lambda <- m_lasso_cv$lambda.min
best_lambda
m_best <- glmnet(x, y, alpha = 1, lambda = best_lambda)
m_best$beta
```

## Regressions

```{r basic-model-allstar, warning = FALSE, message=FALSE}
#Basic model 
m1 <- glm(All.Star ~ player_age + b_ab +  b_total_hits + 
                    b_double + b_triple + b_home_run + b_strikeout +
                    b_bb_percent + AVG300 + slg_percent +
                    on_base_percent + Position,
  data = stats,
  family = "binomial"
)
tidy(m1)
m1_aug <- augment(m1) %>% 
  mutate(prob = exp(.fitted)/(1 + exp(.fitted)),
         pred_leg = ifelse(prob > 0.32, "All-Star", "Not All-Star"))
table(m1_aug$pred_leg, m1_aug$All.Star)
```

```{r stacast-model-all-star, warning = FALSE, message=FALSE}
#Advanced model 
m2 <- glm(All.Star ~ player_age + launch_angle_avg + 
                    barrel + solidcontact_percent + flareburner_percent + 
                    hard_hit_percent + meatball_swing_percent,
  data = stats,
  family = "binomial"
)
tidy(m2)
m2_aug <- augment(m2) %>% 
  mutate(prob = exp(.fitted)/(1 + exp(.fitted)),
         pred_leg = ifelse(prob > 0.32, "All-Star", "Not All-Star"))
table(m2_aug$pred_leg, m2_aug$All.Star)

```

```{r lasso-OBP, warning = FALSE, message=FALSE}
# obp percentage lasso
y <- stats$on_base_percent
x <- model.matrix(on_base_percent ~ launch_angle_avg + sweet_spot_percent + 
                    barrel + solidcontact_percent + flareburner_percent + 
                    hard_hit_percent + avg_hyper_speed + z_swing_percent + 
                    oz_swing_percent + meatball_swing_percent, data = stats)
m_lasso_cv <- cv.glmnet(x, y, alpha = 1)
best_lambda <- m_lasso_cv$lambda.min
best_lambda
m_best <- glmnet(x, y, alpha = 1, lambda = best_lambda)
m_best$beta
```

```{r statcast-obp, warning = FALSE, message=FALSE}
# obp percentage prediction
m3 <- lm(on_base_percent ~ sweet_spot_percent + 
                    barrel + solidcontact_percent + flareburner_percent + 
                    hard_hit_percent + z_swing_percent + 
                    oz_swing_percent + meatball_swing_percent,
  data = stats)

summary(m3)

m3_aug <- augment(m3)
m3_aug |>
ggplot(aes(x = .fitted, y = .resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0, color = "darkred") + 
  labs(x = "Fitted Values", 
       y = "Residual") + 
  theme_bw()
```

```{r slg-lasso, warning = FALSE, message=FALSE}
# slugging percentage lasso
y <- stats$slg_percent
x <- model.matrix(slg_percent ~ launch_angle_avg + sweet_spot_percent + 
                    barrel + solidcontact_percent + flareburner_percent + 
                    hard_hit_percent + avg_hyper_speed + z_swing_percent + 
                    oz_swing_percent + meatball_swing_percent, data = stats)
m_lasso_cv <- cv.glmnet(x, y, alpha = 1)
best_lambda <- m_lasso_cv$lambda.min
best_lambda
m_best <- glmnet(x, y, alpha = 1, lambda = best_lambda)
m_best$beta
```


```{r slg-predict, warning = FALSE, message=FALSE}
# slugging percentage prediction
m4 <- lm(slg_percent ~ launch_angle_avg + sweet_spot_percent + 
                    barrel + solidcontact_percent + flareburner_percent + 
                    hard_hit_percent + avg_hyper_speed + z_swing_percent + 
                    oz_swing_percent + meatball_swing_percent,
  data = stats)

summary(m4)

m4_aug <- augment(m4)
m4_aug |>
ggplot(aes(x = .fitted, y = .resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0, color = "darkred") + 
  labs(x = "Fitted Values", 
       y = "Residual") + 
  theme_bw()
```


```{r}
# Subset for nationals
nationals_stats <- stats |>
  filter(Team == "WAS")

# Predict
pred_obp <- predict(m3, nationals_stats)
pred_slg <- predict(m4, nationals_stats)

# Add to DF
nationals_stats <- nationals_stats |>
  mutate(Predicted_OBP = pred_obp,
         Predicted_SLG = pred_slg)

# Display
print(nationals_stats)
```



ARTICLE ABOUT BATTING ORDER STRATEGY:
https://www.sportsbettingdime.com/guides/strategy/batting-order-sabermetrics/ 